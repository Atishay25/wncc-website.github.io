I"∫<h2 class="display1 m-3 p-3 text-center">Computer Vision Workbench</h2>

<div>
    <img src="/assets/images/soc/item-3.png" width="300" height="300" alt="" class="border rounded img-soc" />
</div>
<div>
    <br />
    <ul>
        
        <li><h4 class="text-primary text-center">Machine Learning</h4></li>
        
        <li><h4 class="text-primary text-center">Image Processing</h4></li>
        
        <li><h4 class="text-primary text-center">Development</h4></li>
        
        <li><h4 class="text-primary text-center">Deep Learning</h4></li>
        
    </ul>
    <br />
    <h4 class="display3  ">Mentors :</h4> 
    <ul>
        
        <li><h5 class=" ">Vishal Kaushal</h5></li>
        
    </ul>
    <h4 class="display3  ">Mentees :</h4> 
    <ul>
        
        <li><h5 class="">Aniket Sadashiva</h5></li>
        
    </ul>
</div>
<div>
    <p class="display3" style="font-size:20px;">
        <br />
        Extracting potentially useful information from videos, like presence of faces, humans, specific objects, motion, tracking etc. is an indispensable part of video analysis.
        <br /><br />
        Recent advancements in deep learning have demonstrated good accuracies on many of these tasks, however these models do not yet generalize well to the unseen real-world deployments.
        <br /><br />
        Though the accuracy numbers for these off the shelf models have been reported and can be computed on standard existing datasets, there is currently no way to estimate how would these models perform when challenged in real-world deployments. This makes it necessary to have at least a qualitative comparison of different models against each other on such challenging, unlabelled videos.
        <br /><br />
        To enable this, we envision a workbench tool (Python GUI) which will make it easy to compare any two models on a given video for a given task and ‚Äúsee‚Äù how they perform. It will be released as open source software. This tool will help understand the ‚Äúreal‚Äù strengths and weaknesses of different models for different tasks and will help give important directions to undertake future research.
        <br /><br />
        To begin with we can focus on the following computer vision tasks: Motion detection, Face detection, Face recognition, Human detection, Head detection, Object detection, Tracking. A typical use of this tool will involve selecting the task, selecting a video, selecting two models/algorithms to compare and as a result seeing a side by side comparison of the analyzed video played synchronously along with other measurable parameters like time taken etc.
    </p>
</div>
<div>
    <h4 class="display3" style="margin:40px 0px 40px 0px;">Tentative Timeline :</h4>
    <table class="table table-striped">
    <thead>
        <tr>
        <th>Week</th>
        <th>Work</th>
        </tr>
    </thead>
    <tbody>
        <tr>
        <td style="width: 120px">Week 1</td>
        <td>Understanding the various computer vision tasks involved</td>
    </tr>
    <tr>
      <td>Week 2</td>
      <td>Motion Detection - stand alone implementations using various alternate techniques</td>
    </tr>
    <tr>
      <td>Week 3</td>
      <td>Face Detection - stand alone implementations using various alternate off the shelf models/techniques</td>
    </tr>
    <tr>
      <td>Week 4</td>
      <td>Object detection - stand alone implementations using various alternate off the shelf models/techniques</td>
    </tr>
    <tr>
      <td>Week 5</td>
      <td>Tool design, architecture and skeleton implementation</td>
    </tr>
    <tr>
      <td>Week 6</td>
      <td>Integrating motion detection in tool</td>
    </tr>
    <tr>
      <td>Week 7</td>
      <td>Integrating face detection in tool</td>
    </tr>
    <tr>
      <td>Week 8</td>
      <td>Integrating object detection in tool</td>
    </tr>
    <tr>
      <td>Week 9</td>
      <td>Face recognition</td>
    </tr>
    <tr>
      <td>Week 10</td>
      <td>Human detection</td>
    </tr>
    <tr>
      <td>Week 11</td>
      <td>Head detection</td>
    </tr>
    <tr>
      <td>Week 12</td>
      <td>Tracking</td>
    </tr>
    <tr>
      <td>Week 13</td>
      <td>Buffer/Enhancements/Closure</td>
    </tr>
    </tbody>
    </table>
</div>

:ET